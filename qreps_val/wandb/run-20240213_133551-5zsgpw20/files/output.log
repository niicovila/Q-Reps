[34m[1mwandb[39m[22m: [33mWARNING[39m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir="...")` before `wandb.init`
/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(terminated, (bool, np.bool8)):
/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/memory/replay_buffer.py:47: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:264.)
  obs_tm1 = torch.tensor(obs_tm1).float()
[2024-02-13 13:35:58,751]: Iteration 0 done, Reward 0.06
[2024-02-13 13:35:59,227]: Iteration 1 done, Reward 0.04
[2024-02-13 13:35:59,748]: Iteration 2 done, Reward 0.05
[2024-02-13 13:36:00,143]: Iteration 3 done, Reward 0.04
[2024-02-13 13:36:00,602]: Iteration 4 done, Reward 0.04
[2024-02-13 13:36:01,010]: Iteration 5 done, Reward 0.04
[2024-02-13 13:36:01,391]: Iteration 6 done, Reward 0.04
[2024-02-13 13:36:01,760]: Iteration 7 done, Reward 0.04
[2024-02-13 13:36:02,159]: Iteration 8 done, Reward 0.04
[2024-02-13 13:36:02,541]: Iteration 9 done, Reward 0.04
[2024-02-13 13:36:02,905]: Iteration 10 done, Reward 0.05
[2024-02-13 13:36:03,263]: Iteration 11 done, Reward 0.03
[2024-02-13 13:36:03,617]: Iteration 12 done, Reward 0.05
[2024-02-13 13:36:03,989]: Iteration 13 done, Reward 0.04
[2024-02-13 13:36:04,343]: Iteration 14 done, Reward 0.05
[2024-02-13 13:36:04,743]: Iteration 15 done, Reward 0.04
[2024-02-13 13:36:05,142]: Iteration 16 done, Reward 0.03
[2024-02-13 13:36:05,493]: Iteration 17 done, Reward 0.04
Traceback (most recent call last):
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/experiments/cartpole.py", line 120, in <module>
    train(qreps_config)
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/experiments/cartpole.py", line 116, in train
    trainer.train(num_iterations=50, max_steps=500, number_rollouts=5)
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/utilities/trainer.py", line 62, in train
    self.algo.update_policy(self.iter)
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/algorithms/qreps.py", line 124, in update_policy
    self.optimize_loss(self.dual, optimizer=self.theta_opt)
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/algorithms/abstract_algorithm.py", line 166, in optimize_loss
    optimizer.step(closure)
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/optim/optimizer.py", line 373, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/optim/adam.py", line 143, in step
    loss = closure()
           ^^^^^^^^^
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/algorithms/abstract_algorithm.py", line 161, in closure
    loss = loss_fn(observations, next_observations, rewards, actions)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/algorithms/qreps.py", line 99, in dual
    return empirical_logistic_bellman(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/utilities/elbe.py", line 26, in empirical_logistic_bellman
    * empirical_bellman_error(
      ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/utilities/elbe.py", line 17, in empirical_bellman_error
    q_features = q_func(features, actions)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/valuefunctions/q_function.py", line 27, in forward
    model_output = self.forward_state(observation)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/workplace/uni/QREPS_ORIGINAL/tests/qreps_val/qreps/valuefunctions/q_function.py", line 43, in forward_state
    return self.model(input)
           ^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/nn/modules/container.py", line 215, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/nicolasvila/miniconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt